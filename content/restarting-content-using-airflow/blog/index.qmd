---
title: Machine Learning Development with Posit Connect and Airflow
---

This tutorial demonstrates using Posit Connect and Apache Airflow to implement a simple machine learning development lifecycle. Wow, that was a mouthful. Let's break it down.

Apache Airflow is self-described as ["a platform created by the community to programmatically author, schedule, and monitor workflows"](https://airflow.apache.org). In other words, we can use Apache Airflow to orchestrate custom workflows. This includes, but is not limited to, a machine learning development workflow.

A machine learning development workflow is designed, in part, to create a machine learning model systematicaly. A robust workflow often has many additional components, but for this tutorial, we will concentrate on producing a model that we can use to perform a classification.

After training the model, we will deploy an application to Connect that uses the model to predict a classification based on user input.

Then, we will automate our machine learning development workflow for Apache Airflow and programatically update our application to use different models.

## Machine Learning Development Workflow

### Data Exploration

The first step in our machine learning development workflow is data exploration. For this tutorial we will use the [`palmerpenguins`](https://allisonhorst.github.io/palmerpenguins/) dataset.

To get started, import the penguins dataset from [`seaborn`](https://seaborn.pydata.org) and understand it's shape.

```{python}
import seaborn as sns

penguins = sns.load_dataset('penguins')
```

```{python}
# | echo: false
penguins.info()
```

Great! The dataframe contains 344 entries and 7 columns.

Let's try to build a machine learning model which correctly determines the species using other information avaiable in the dataset. The `species` column contains three unique values: *Adelie*, *Chinstrap*, and *Gentoo*. For simplicity, let's use the four numerical columns as features in our dataset: `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`.

::: {.callout-tip}
Check out the ["Introduction to palmerpenguins"](https://allisonhorst.github.io/palmerpenguins/articles/intro.html) to learn more about the dataset.
:::

### Feature Extraction

Let's begin by cleaning by preparing the dataset for model training.

First, let's remove missing values from the dataset.

```{python}
penguins = penguins.dropna()
```

Then we'll use a `LabelEncoder` to encode the values of the `species` column as integers.

```{python}
from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
penguins["species"] = encoder.fit_transform(penguins["species"])
```

Let's extract the relevant features and target value into a conventional `X` and `y` dataframe.

```{python}
X = penguins[["bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g"]]
y = penguins["species"]
```

Next, split `X` and `y` into training and testing sets. We'll use to the testing set to evaulate the performance of our model.

```{python}
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=0
)
```

### Model Training

Let's train a simple logistic regression model and evaluate its performance. We'll use a `StandardScaler` to normalize the distribution of each column in the feature sets. This is necessary to reduce bias among features.

```{python}
# | output: false
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

pipeline = Pipeline([("scaler", StandardScaler()), ("model", LogisticRegression())])
pipeline = pipeline.fit(X_train, y_train)
```

### Model Evaluation

Here we predict the `species` of penguins in the `X_test` dataset.

```{python}
y_pred = pipeline.predict(X_test)
```

Let's use a confusion matrix to compare the predicted `species` (`y_pred`) against the true `species` (`y_test`).

```{python}
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=encoder.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.show()
```

The confusion matrix shows the cross-section of predicted and true classifications. In this example, the model incorrectly classified 2 *Chinstrap* penguins as *Adelie* penguins.

### Model Saving

Finally, let's write the encoder and pipeline to Connect. Check out the `pins` documentation for additional information on configuring a `board_connect` with your credentials.

```python
import pins

board = pins.board_connect(allow_pickle_read=True)
board.pin_write(encoder, name="taylor_steinberg/encoder", type="joblib")
board.pin_write(pipeline, name="taylor_steinberg/pipeline", type="joblib")
```

Now that we have trained and deployed our model to Connect, let's build a Shiny application that predicts a penguin species based on user input.

::: {.callout-warning}
Storing machine learning models as `pins` works well for small models. But, larger models may require an alternative storage solution. `pins` supports a variety of storage solutions. Check out the [`pins` documentation](https://rstudio.github.io/pins-python/reference/) to learn more.
:::

## Create an Application

Let's create a simple [`shiny`](https://shiny.posit.co/py/index.html) application and publish it on Connect. This example loads the dataset from `encoder` and `pipeline` files created above as Connect Pins. We then collect input from the user and then call the `pipeline.predict` method to determine the species based on the input information. Since the prediction produces a numeric value, we use the encoder to convert the numeric value back to the human-readable `species`.

```python
# app.py
import joblib
import pandas as pd
import pins
from shiny.express import input, render, ui

board = pins.board_connect(allow_pickle_read=True)
encoder = board.pin_read("taylor_steinberg/encoder")
pipeline = board.pin_read("taylor_steinberg/pipeline")

with ui.layout_sidebar():
    with ui.sidebar():
        ui.input_slider("bill_length_mm", "Bill Length (mm)", 25, 65, value=45),
        ui.input_slider("bill_depth_mm", "Bill Depth (mm)", 10, 25, value=17),
        ui.input_slider("flipper_length_mm", "Flipper Length (mm)", 160, 240, value=200),
        ui.input_slider("body_mass_g", "Body Mass (g)", 2500, 6500, value=4000),


    @render.text
    def f():
        bill_length_mm = float(input.bill_length_mm())
        bill_depth_mm = float(input.bill_depth_mm())
        flipper_length_mm = float(input.flipper_length_mm())
        body_mass_g = float(input.body_mass_g())
        df = pd.DataFrame(
            [[bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g]],
            columns=["bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g"],
        )
        species = pipeline.predict(df)
        species = encoder.inverse_transform(species)
        return f"""
        I think this is a {str(species[0])} penguin.
        """
```

Start your Shiny application using `shiny run app.py`. Here's the application running on my machine. I added to our example to highlight the selected penguin from this image created by [Allison Horst](https://github.com/allisonhorst/palmerpenguins/blob/c19a904462482430170bfe2c718775ddb7dbb885/man/figures/lter_penguins.png)!

![](app.gif){width=100%}

Let's publish this application to Connect using the [Posit Publisher extension](https://open-vsx.org/extension/posit/publisher) for VSCode and Positron. I named my application "Penguin Predictions". You can name your application anything you like.

::: {.callout-warning}
At the time of writing, the Posit Publisher extension is avaiable as a pre-release. For alternative publishing solutions, see the [Posit Connect Documentation](https://docs.posit.co/connect/user/publishing-overview/).
:::

![](publish.gif){width=100%}

## Airflow

### Overview

Now that we have developed an end-to-end machine learning workflow let's automate it using Airflow. In this section, we will create an [Airflow DAG](#airflow-dag) to automate our machine learning workflow. This DAG is broken down following steps:

- [Data Preparation](#airflow-data-preparation)
- [Model Training](#airflow-model-training)
- [Model Evaluation](#airflow-model-evaluation)
- [Model Deployment](#airflow-model-deployment)
- [Application Restart](#airflow-application-restart)

In our example, we will keep things simple, but here are a few examples of how different eventing scenarios can be utilized to accomodate your organizations needs.

- Schedule
   - This is the most common event trigger in Airflow. You can use schedules to execute your workflow periodically.
   - *Example: You want to retrain your model daily using the latest available dataset.*
- Tasks
   - Monitor another Airflow DAG and start your workflow when it is complete.
   - *Example: Your dataset is produced by an ETL coordinated in a separate Airflow DAG, and you want to retrain your model once the ETL completes.*
- Database Sensors
   - Monitor a database table and trigger your workflow based on specific changes or conditions in the table.
   - *Example: You develop a long-running hyperparameter tuning and cross-validation workflow that writes model scores to a database table. You want to monitor the table and send a Slack notification whenever an improved model is identified.*

You can see how Airflow can quickly improve our machine-learning workflow to accommodate our organization's needs.

### Data Preparation {#airflow-data-preparation}

The first step in our Airflow DAG is to prepare the data. We will load the `palmerpenguins` dataset, clean it by removing missing values, and encode the species column.

```python
import datetime
import seaborn as sns
from airflow.decorators import dag, task
from sklearn.preprocessing import LabelEncoder

@task
def prepare_data():
    penguins = sns.load_dataset("penguins", cache=False)
    penguins = penguins.dropna()
    encoder = LabelEncoder()
    penguins["species"] = encoder.fit_transform(penguins["species"])
    return {"penguins": penguins, "encoder": encoder}
```
### Model Training {#airflow-model-training}

Next, we will train a logistic regression model using the prepared data. We will split the data into training and testing sets, create a pipeline with a standard scaler and logistic regression model, and fit the model.

```python
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

@task
def train_model(data):
    penguins = data["penguins"]
    encoder = data["encoder"]
    X = penguins[
        ["bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g"]
    ]
    y = penguins["species"]
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=0
    )
    pipeline = Pipeline([("scaler", StandardScaler()), ("model", LogisticRegression())])
    pipeline.fit(X_train, y_train)
    return {"pipeline": pipeline, "encoder": encoder, "X_test": X_test, "y_test": y_test}
```

### Model Evaluation {#airflow-model-evaluation}

We will evaluate the trained model by calculating its accuracy score on the test set. If the model meets a specified threshold, we will proceed to deploy it.

```python
from sklearn.metrics import accuracy_score

@task
def evaluate_model(data):
    pipeline = data["pipeline"]
    X_test = data["X_test"]
    y_test = data["y_test"]
    score = pipeline.score(X_test, y_test)
    print(f"Model score: {score}")
    if score < 0.9:
        print("Model score does not meet 90% threshold")
        return False
    print("Model score meets 90% threshold")
    return True
```

### Model Deployment {#airflow-model-deployment}

If the model evaluation is successful, we will deploy the model and encoder using Posit Connect. We will use the pins package to write the encoder and pipeline to Connect.

```python
import pins

CONNECT_SERVER = Variable.get("CONNECT_SERVER")
CONNECT_API_KEY = Variable.get("CONNECT_API_KEY")
CONNECT_USER_NAME = "USER_NAME"

@task
def deploy_model(data, is_valid):
    if not is_valid:
        print("Skipping deployment")
        return False
    print("Deploying model")
    encoder = data["encoder"]
    pipeline = data["pipeline"]
    board = pins.board_connect(CONNECT_SERVER, api_key=CONNECT_API_KEY, cache=None, allow_pickle_read=True)
    board.pin_write(encoder, name=f"{CONNECT_USER_NAME}/encoder", type="joblib")
    board.pin_write(pipeline, name=f"{CONNECT_USER_NAME}/pipeline", type="joblib")
    return True
```

### Application Restart {#airflow-application-restart}

Finally, if the model is successfully deployed, we will restart our Shiny application on Posit Connect to use the updated model.

```python
from posit import connect

CONNECT_CONTENT_GUID = "CONTENT_GUID"

@task
def restart_app(is_deployed):
    if not is_deployed:
        print("Skipping restart")
        return
    client = connect.Client(CONNECT_SERVER, CONNECT_API_KEY)
    content = client.content.get(CONNECT_CONTENT_GUID)
    print(f"Restarting {content.dashboard_url}")
    content.restart()
```

### Defining the DAG {#airflow-dag}

Now, we will define our Airflow DAG to include all the tasks we created. The DAG will run daily and execute each task in sequence.

```python
@dag(catchup=False, start_date=datetime.datetime(2021, 1, 1), schedule="@daily")
def penguins_dag():
    data = prepare_data()
    model_data = train_model(data)
    is_valid = evaluate_model(model_data)
    is_deployed = deploy_model(model_data, is_valid)
    restart_app(is_deployed)
```

### Full Example

```python
import datetime

import pins
import seaborn as sns
from airflow.decorators import dag, task
from airflow.models import Variable
from posit import connect
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder, StandardScaler

CONNECT_SERVER = Variable.get("CONNECT_SERVER")
CONNECT_API_KEY = Variable.get("CONNECT_API_KEY")
CONNECT_USER_NAME = "USER_NAME"
CONNECT_CONTENT_GUID = "CONTENT_GUID"


@task
def prepare_data():
    penguins = sns.load_dataset("penguins", cache=False)
    penguins = penguins.dropna()
    encoder = LabelEncoder()
    penguins["species"] = encoder.fit_transform(penguins["species"])
    return {"penguins": penguins, "encoder": encoder}


@task
def train_model(data):
    penguins = data["penguins"]
    encoder = data["encoder"]
    X = penguins[
        ["bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g"]
    ]
    y = penguins["species"]
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=0
    )
    pipeline = Pipeline([("scaler", StandardScaler()), ("model", LogisticRegression())])
    pipeline.fit(X_train, y_train)
    return {"pipeline": pipeline, "encoder": encoder, "X_test": X_test, "y_test": y_test}


@task
def evaluate_model(data):
    pipeline = data["pipeline"]
    X_test = data["X_test"]
    y_test = data["y_test"]
    score = pipeline.score(X_test, y_test)
    print(f"Model score: {score}")
    if score < 0.9:
        print("Model score does not meet 90% threshold")
        return False
    print("Model score meets 90% threshold")
    return True


@task
def deploy_model(data, is_valid):
    if not is_valid:
        print("Skipping deployment")
        return False
    print("Deploying model")
    encoder = data["encoder"]
    pipeline = data["pipeline"]
    board = pins.board_connect(CONNECT_SERVER, api_key=CONNECT_API_KEY, cache=None, allow_pickle_read=True)
    board.pin_write(encoder, name=f"{CONNECT_USER_NAME}/encoder", type="joblib")
    board.pin_write(pipeline, name=f"{CONNECT_USER_NAME}/pipeline", type="joblib")
    return True


@task
def restart_app(is_deployed):
    if not is_deployed:
        print("Skipping restart")
        return
    client = connect.Client(CONNECT_SERVER, CONNECT_API_KEY)
    content = client.content.get(CONNECT_CONTENT_GUID)
    print(f"Restarting {content.dashboard_url}")
    content.restart()


@dag(catchup=False, start_date=datetime.datetime(2021, 1, 1), schedule="@daily")
def penguins_dag():
    data = prepare_data()
    model_data = train_model(data)
    is_valid = evaluate_model(model_data)
    is_deployed = deploy_model(model_data, is_valid)
    restart_app(is_deployed)


penguins_dag()
```

## Conclusion

This tutorial provides a guide to implementing a machine learning development lifecycle using Posit Connect and Apache Airflow. You can orchestrate your machine learning workflows, from data exploration to model training and deployment, leveraging Airflow's scheduling capabilities for automation and integration. Use this approach to scale your machine learning development and provide meaningful insights to your organization using Posit Connect. Happy coding!
